{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88EvRVo3GZ6D"
      },
      "source": [
        "### libs and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w47DU8wogel8",
        "outputId": "58e3cccb-574a-420a-879c-a123f6f339f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyRDDLGym\n",
            "  Downloading pyrddlgym-2.6-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: ply in /usr/local/lib/python3.12/dist-packages (from pyRDDLGym) (3.11)\n",
            "Requirement already satisfied: pillow>=9.2.0 in /usr/local/lib/python3.12/dist-packages (from pyRDDLGym) (11.3.0)\n",
            "Requirement already satisfied: matplotlib>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from pyRDDLGym) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.12/dist-packages (from pyRDDLGym) (2.0.2)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (from pyRDDLGym) (1.2.3)\n",
            "Collecting pygame-ce (from pyRDDLGym)\n",
            "  Downloading pygame_ce-2.5.6-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from pyRDDLGym) (3.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5.0->pyRDDLGym) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5.0->pyRDDLGym) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5.0->pyRDDLGym) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5.0->pyRDDLGym) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5.0->pyRDDLGym) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5.0->pyRDDLGym) (3.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5.0->pyRDDLGym) (2.9.0.post0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium->pyRDDLGym) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium->pyRDDLGym) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium->pyRDDLGym) (0.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.5.0->pyRDDLGym) (1.17.0)\n",
            "Downloading pyrddlgym-2.6-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pygame_ce-2.5.6-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (12.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pygame-ce, pyRDDLGym\n",
            "Successfully installed pyRDDLGym-2.6 pygame-ce-2.5.6\n",
            "Collecting rddlrepository\n",
            "  Downloading rddlrepository-2.1-py3-none-any.whl.metadata (959 bytes)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rddlrepository) (2.0.2)\n",
            "Downloading rddlrepository-2.1-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rddlrepository\n",
            "Successfully installed rddlrepository-2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyRDDLGym\n",
        "import pyRDDLGym\n",
        "!pip install rddlrepository\n",
        "\n",
        "from rddlrepository.core.manager import RDDLRepoManager\n",
        "manager = RDDLRepoManager(rebuild=True)\n",
        "\n",
        "from IPython.display import Image # for displaying gifs in colab\n",
        "\n",
        "base_path = '/content/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgxqsGNpokOl"
      },
      "source": [
        "###Our domain file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiEOhsioj7Fk",
        "outputId": "38e1677f-0a0f-439b-de8d-def60f3bf259"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated RDDL file written successfully!\n"
          ]
        }
      ],
      "source": [
        "base_path = '/content/'\n",
        "\n",
        "TRAIN_DOMAIN = \"\"\"\n",
        "domain train_system {\n",
        "\n",
        "    requirements = {\n",
        "        concurrent,\n",
        "        reward-deterministic,\n",
        "        intermediate-nodes,\n",
        "        constrained-state\n",
        "    };\n",
        "\n",
        "    types {\n",
        "        train: object;\n",
        "        station: object;\n",
        "    };\n",
        "\n",
        "    pvariables{\n",
        "\n",
        "        //train constants\n",
        "        CAPACITY(train): {non-fluent, int, default=1000};\n",
        "        PASSENGERS_BOARDING_PER_MINUTE(train): {non-fluent, int, default=75};\n",
        "        PASSENGERS_DISEMBARKING_PER_MINUTE(train): {non-fluent, int, default=75};\n",
        "        NEXT_TRAIN(train, train): {non-fluent, bool, default=false};\n",
        "        MIN_STOP_TIME(train): {non-fluent, real, default=2.0};\n",
        "\n",
        "        //station constants\n",
        "        INF : {non-fluent, real, default=1000000.0};\n",
        "        FIRST_TRAIN : {non-fluent, train};\n",
        "        DEPOT_STATION(station): {non-fluent, bool, default=false};\n",
        "        NEXT_STATION(station, station): {non-fluent, bool, default=false};\n",
        "        DRIVE_TIME(station, station): {non-fluent, real, default=0.0};\n",
        "        FIND_NEXT_STATION(station): {non-fluent, station};\n",
        "        DISEMBARKING_PRECENTAGE(station): {non-fluent, real, default=0.2};\n",
        "        PASSENGER_ARRIVAL_RATE(station): {non-fluent, int, default=6};\n",
        "        PLANNED_DEPARTURE_TIME(train, station): {non-fluent, real, default=0.0};\n",
        "\n",
        "        //timing constants\n",
        "        SCHEDULED_DEPARTURE_TIME(train, station): {non-fluent, real, default=0.0};\n",
        "\n",
        "        //timer variables\n",
        "        train_timer(train): {state-fluent, real, default=0.0};\n",
        "\n",
        "        // --- NEW: Absolute Clock ---\n",
        "        current_time: {state-fluent, real, default=0.0};\n",
        "\n",
        "        //interm fluent\n",
        "        active_time_calc(train): {interm-fluent, real};\n",
        "        people_boarding_calc(train, station): {interm-fluent, real};\n",
        "        people_disembarking_calc(train, station): {interm-fluent, real};\n",
        "        entering_empty_queue(train): {interm-fluent, bool};\n",
        "        leaving_queue(train): {interm-fluent, bool};\n",
        "        global_timer: {interm-fluent, real};\n",
        "\n",
        "        // train state enum\n",
        "        TRAIN_IN_ROUTE   : {non-fluent, int, default=0};\n",
        "        TRAIN_IN_QUEUE   : {non-fluent, int, default=1};\n",
        "        TRAIN_WAITING    : {non-fluent, int, default=2};\n",
        "        TRAIN_ACTIVE     : {non-fluent, int, default=3};\n",
        "        TRAIN_FINISHED   : {non-fluent, int, default=4};\n",
        "\n",
        "        //state variables - passengers\n",
        "        passengers_at_station(station): {state-fluent, real, default=0};\n",
        "        passengers_on_train(train): {state-fluent, real, default=0};\n",
        "\n",
        "        //state variables - train states\n",
        "        train_num_at_queue(train): {state-fluent, real, default=-1};\n",
        "        current_station(train): {state-fluent, station};\n",
        "        current_state(train): {state-fluent, int, default=0};\n",
        "\n",
        "        //actions (RENAMED from wait_action -> wait)\n",
        "        wait(station): {action-fluent, int, default=0};\n",
        "\n",
        "        //action-interm (RENAMED from wait -> delay)\n",
        "        delay(train) : {interm-fluent, int};\n",
        "    };\n",
        "\n",
        "    cpfs {\n",
        "\n",
        "        // (RENAMED: delay depends on wait)\n",
        "        delay(?t) = wait(current_station(?t));\n",
        "\n",
        "        global_timer() = min_{?t: train} [train_timer(?t)];\n",
        "\n",
        "        current_time'() = current_time() + global_timer();\n",
        "\n",
        "        leaving_queue(?t) =  current_state(?t) == TRAIN_ACTIVE ^ train_timer(?t) == global_timer();\n",
        "\n",
        "        entering_empty_queue(?t) = (FIRST_TRAIN == ?t) | exists_{?t2: train} [NEXT_TRAIN(?t2,?t) ^ ((leaving_queue(?t2) ^ (current_station(?t) == current_station(?t2))) | ~(current_station(?t) == current_station(?t2)))];\n",
        "\n",
        "        people_disembarking_calc(?t,?s) = floor[DISEMBARKING_PRECENTAGE(?s) * passengers_on_train(?t)];\n",
        "\n",
        "        people_boarding_calc(?t,?s) = abs[min[CAPACITY(?t) - passengers_on_train(?t) + people_disembarking_calc(?t,?s), passengers_at_station(?s)]];\n",
        "\n",
        "        active_time_calc(?t) = max[MIN_STOP_TIME(?t),\n",
        "                                   ceil[sum_{?s: station}[[current_station(?t) == ?s ^ (current_state(?t) == TRAIN_WAITING | current_state(?t) == TRAIN_IN_ROUTE)] *\n",
        "                                           [people_disembarking_calc(?t,?s) + people_boarding_calc(?t,?s)] /\n",
        "                                            PASSENGERS_DISEMBARKING_PER_MINUTE(?t)]]];\n",
        "\n",
        "        passengers_at_station'(?s) =\n",
        "                                if(DEPOT_STATION(?s))\n",
        "                                    then 0\n",
        "                                else if(exists_{?t: train} [current_station(?t) == ?s ^ current_state(?t) == TRAIN_IN_ROUTE ^ train_timer(?t) == global_timer()])\n",
        "                                    then passengers_at_station(?s) + PASSENGER_ARRIVAL_RATE(?s) -\n",
        "                                      sum_{?t: train} [(current_station(?t) == ?s ^ current_state(?t) == TRAIN_ACTIVE) * people_boarding_calc(?t,?s)]\n",
        "                                else passengers_at_station(?s) + PASSENGER_ARRIVAL_RATE(?s);\n",
        "\n",
        "        passengers_on_train'(?t) =\n",
        "                                if(exists_{?s: station} [(current_station(?t) == ?s ^ current_state(?t) == TRAIN_ACTIVE) ^ train_timer(?t) == global_timer()])\n",
        "                                    then passengers_on_train(?t) +\n",
        "                                         sum_{?s: station}[(current_station(?t) == ?s ^ current_state(?t) == TRAIN_ACTIVE) *\n",
        "                                                         [people_boarding_calc(?t,?s) - people_disembarking_calc(?t,?s)]]\n",
        "                                else passengers_on_train(?t);\n",
        "\n",
        "        current_state'(?t) =\n",
        "                              if(current_state(?t) == TRAIN_FINISHED)\n",
        "                                    then TRAIN_FINISHED\n",
        "                              else if(exists_{?s: station} [(current_station(?t) == ?s ^ current_state(?t) == TRAIN_IN_ROUTE) ^ train_timer(?t) == global_timer() ^ DEPOT_STATION(?s)] | train_timer(?t) == INF)\n",
        "                                    then TRAIN_FINISHED\n",
        "                              else if(current_state(?t) == TRAIN_ACTIVE ^ (train_timer(?t) == global_timer()))\n",
        "                                    then TRAIN_IN_ROUTE\n",
        "\n",
        "                              // (RENAMED wait -> delay)\n",
        "                              else if(current_state(?t) == TRAIN_IN_ROUTE ^ (train_timer(?t) == global_timer()) ^ (delay(?t) > 0) ^ entering_empty_queue(?t))\n",
        "                                    then TRAIN_WAITING\n",
        "                              else if((current_state(?t) == TRAIN_IN_QUEUE) ^ delay(?t) > 0 ^\n",
        "                                                    (exists_{?t2: train} [NEXT_TRAIN(?t2,?t) ^ (current_station(?t2) == current_station(?t)) ^ leaving_queue(?t2)]))\n",
        "                                    then TRAIN_WAITING\n",
        "                              else if(current_state(?t) == TRAIN_IN_ROUTE ^ (train_timer(?t) == global_timer()) ^ (delay(?t) == 0) ^ entering_empty_queue(?t))\n",
        "                                    then TRAIN_ACTIVE\n",
        "                              else if((current_state(?t) == TRAIN_IN_QUEUE) ^ (delay(?t) == 0) ^\n",
        "                                                    exists_{?t2: train} [NEXT_TRAIN(?t2,?t) ^ (current_station(?t2) == current_station(?t)) ^ leaving_queue(?t2)])\n",
        "                                    then TRAIN_ACTIVE\n",
        "                              else if(current_state(?t) == TRAIN_WAITING ^ (train_timer(?t) == global_timer()))\n",
        "                                    then TRAIN_ACTIVE\n",
        "                              else if(current_state(?t) == TRAIN_IN_ROUTE ^ (train_timer(?t) == global_timer()) ^\n",
        "                                     exists_{?t2: train} [NEXT_TRAIN(?t2,?t) ^ (current_station(?t2) == current_station(?t)) ^ (train_num_at_queue(?t2) >= 0.0) ^ ~(train_timer(?t2) == global_timer())])\n",
        "                                    then TRAIN_IN_QUEUE\n",
        "                              else current_state(?t);\n",
        "\n",
        "      current_station'(?t) = if((current_state(?t) == TRAIN_ACTIVE) ^ (train_timer(?t) == global_timer()))\n",
        "                                 then FIND_NEXT_STATION(current_station(?t))\n",
        "                             else current_station(?t);\n",
        "\n",
        "        train_num_at_queue'(?t) = if((current_state(?t) == TRAIN_IN_ROUTE) ^ (train_timer(?t) == global_timer()))\n",
        "                                    then[\n",
        "                                         if(exists_{?s: station} [DEPOT_STATION(?s) ^ current_station(?t) == ?s])\n",
        "                                            then -1.0\n",
        "                                         else if(exists_{?t2: train} [NEXT_TRAIN(?t2,?t) ^ ~(current_station(?t) == current_station(?t2))])\n",
        "                                            then 0.0\n",
        "                                         else\n",
        "                                            sum_{?t2: train}[NEXT_TRAIN(?t2,?t) * (train_num_at_queue(?t2) + 1.0 -\n",
        "                                            sum_{?t3: train}[(current_station(?t) == current_station(?t3)) ^ current_state(?t3) == TRAIN_ACTIVE ^ train_timer(?t3) == global_timer()])]]\n",
        "                                    else if(train_num_at_queue(?t) > 0.0 ^ exists_{?t2: train} [current_station(?t) == current_station(?t2) ^\n",
        "                                                                        train_num_at_queue(?t2) == 0.0 ^\n",
        "                                                                        train_timer(?t2) == global_timer() ^\n",
        "                                                                        current_state(?t2) == TRAIN_ACTIVE])\n",
        "                                        then train_num_at_queue(?t) - 1.0\n",
        "                                    else if(train_num_at_queue(?t) == 0.0 ^ current_state(?t) == TRAIN_ACTIVE ^ train_timer(?t) == global_timer())\n",
        "                                        then -1.0\n",
        "                                    else train_num_at_queue(?t);\n",
        "\n",
        "        train_timer'(?t) =\n",
        "                    if(current_state(?t) == TRAIN_FINISHED | exists_{?s: station} [(current_station(?t) == ?s) ^ (current_state(?t) == TRAIN_IN_ROUTE) ^ (train_timer(?t) == global_timer()) ^ DEPOT_STATION(?s)])\n",
        "                      then INF\n",
        "                    // (RENAMED wait -> delay)\n",
        "                    else if(exists_{?s: station} [(current_station(?t) == ?s) ^ (current_state(?t) == TRAIN_IN_ROUTE) ^ (train_timer(?t) == global_timer()) ^ (delay(?t) == 0) ^ entering_empty_queue(?t)])\n",
        "                      then active_time_calc(?t)\n",
        "                    else if(exists_{?s: station} [(current_station(?t) == ?s) ^ (train_num_at_queue(?t) == 1) ^ (delay(?t) == 0) ^\n",
        "                          exists_{?t2: train} [NEXT_TRAIN(?t2,?t) ^ (current_station(?t2) == ?s) ^ leaving_queue(?t2)]])\n",
        "                      then active_time_calc(?t)\n",
        "                    else if(exists_{?s: station} [(current_station(?t) == ?s) ^ (current_state(?t) == TRAIN_WAITING)] ^ (train_timer(?t) == global_timer()))\n",
        "                      then active_time_calc(?t)\n",
        "                    else if(exists_{?s: station} [(current_station(?t) == ?s) ^ (current_state(?t) == TRAIN_IN_ROUTE) ^ (train_timer(?t) == global_timer()) ^ (delay(?t) > 0) ^ entering_empty_queue(?t)])\n",
        "                      then delay(?t)\n",
        "                    else if(exists_{?s: station} [(current_station(?t) == ?s) ^ (train_num_at_queue(?t) == 1) ^ (delay(?t) > 0) ^\n",
        "                            exists_{?t2: train} [NEXT_TRAIN(?t2,?t) ^ (current_station(?t2) == ?s) ^ leaving_queue(?t2)]])\n",
        "                      then round[delay(?t)]\n",
        "                    else if(exists_{?s: station} [(current_station(?t) == ?s) ^ (current_state(?t) == TRAIN_ACTIVE)] ^ (train_timer(?t) == global_timer()))\n",
        "                      then sum_{?s: station, ?s2: station} [(current_station(?t) == ?s) * (current_state(?t) == TRAIN_ACTIVE) * NEXT_STATION(?s,?s2) * DRIVE_TIME(?s,?s2)]\n",
        "                    else max[train_timer(?t) - global_timer() , 0.0];\n",
        "\n",
        "    };\n",
        "\n",
        "reward = -1 * abs[sum_{?t: train} [\n",
        "        (current_state(?t) == TRAIN_ACTIVE ^ train_timer(?t) == global_timer()) *\n",
        "        (PLANNED_DEPARTURE_TIME(?t, current_station(?t)) - (current_time + global_timer()))\n",
        "    ]];\n",
        "\n",
        "action-preconditions {\n",
        "          // (RENAMED wait_action -> wait)\n",
        "          forall_{?s : station} wait(?s) >= 0;\n",
        "          forall_{?s : station} wait(?s) <= 10;\n",
        "      };\n",
        "\n",
        "    state-invariants {\n",
        "\n",
        "    };\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "domain_file = open(base_path+'domain.rddl','w')\n",
        "domain_file.write(TRAIN_DOMAIN)\n",
        "domain_file.close()\n",
        "print(\"Updated RDDL file written successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsf8EJjop5bG"
      },
      "source": [
        "### Our instance files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CU3O3LNoWls"
      },
      "source": [
        "#### instance generator - implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwGRasvUof94"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "def generate_pyrddlgym_instance(num_trains, num_stations, horizon, interval=30, start_delay=10, train_capacity=1000, boarding_speed=75):\n",
        "    if num_trains < 2 or num_stations < 2:\n",
        "        raise ValueError(\"There must be at least 2 trains and 2 stations.\")\n",
        "\n",
        "    trains = [f\"t{i}\" for i in range(1, num_trains + 1)]\n",
        "    stations = [f\"s{i}\" for i in range(1, num_stations + 1)]\n",
        "\n",
        "    # Define random drive times\n",
        "    drive_times = {\n",
        "        (stations[i], stations[i + 1]): random.randint(10, 60)\n",
        "        for i in range(len(stations) - 1)\n",
        "    }\n",
        "\n",
        "    # t1 starts at 'start_delay', t2 at 'start_delay + interval', etc.\n",
        "    # This allows passengers to accumulate at s1 before t1 arrives.\n",
        "    train_timers = [start_delay + (i * interval) for i in range(num_trains)]\n",
        "\n",
        "    # Disembarking Percentages\n",
        "    station_weights = [random.randint(1, 10) for _ in range(num_stations)]\n",
        "    total_weight = sum(station_weights)\n",
        "    if total_weight == 0: total_weight = 1\n",
        "\n",
        "    disembarking_percentages = {\n",
        "        stations[i]: station_weights[i] / total_weight\n",
        "        for i in range(num_stations)\n",
        "    }\n",
        "\n",
        "    # Passenger Arrival Rates\n",
        "    # Last station (Depot) gets 0 arrival rate. s1 gets a normal random rate.\n",
        "    passenger_arrival_rates = {}\n",
        "    for i, station in enumerate(stations):\n",
        "        if i == len(stations) - 1: # Last station\n",
        "            passenger_arrival_rates[station] = 0\n",
        "        else:\n",
        "            passenger_arrival_rates[station] = random.randint(2, 8)\n",
        "\n",
        "    # --- SIMULATION START ---\n",
        "    planned_departures = {}\n",
        "\n",
        "    station_free_time = {s: 0.0 for s in stations}\n",
        "    station_last_snapshot_time = {s: 0.0 for s in stations}\n",
        "    station_leftover_passengers = {s: 0.0 for s in stations}\n",
        "\n",
        "    for t_idx, train in enumerate(trains):\n",
        "        current_train_load = 0.0\n",
        "\n",
        "        # 1. Determine Natural Arrival at s1 (includes the start_delay now)\n",
        "        natural_arrival_time = train_timers[t_idx]\n",
        "\n",
        "        for s_idx, station in enumerate(stations):\n",
        "            if s_idx > 0:\n",
        "                prev_station = stations[s_idx - 1]\n",
        "                drive = drive_times[(prev_station, station)]\n",
        "                natural_arrival_time += drive\n",
        "\n",
        "            # --- SNAPSHOT LOGIC ---\n",
        "            # For t1 at s1, accumulation_interval will be 'start_delay' (e.g. 10 - 0 = 10 mins).\n",
        "            accumulation_interval = natural_arrival_time - station_last_snapshot_time[station]\n",
        "            if accumulation_interval < 0: accumulation_interval = 0\n",
        "\n",
        "            new_arrivals = accumulation_interval * passenger_arrival_rates[station]\n",
        "            total_waiting = new_arrivals + station_leftover_passengers[station]\n",
        "\n",
        "            station_last_snapshot_time[station] = natural_arrival_time\n",
        "\n",
        "            # --- BLOCKING LOGIC ---\n",
        "            actual_boarding_start = max(natural_arrival_time, station_free_time[station])\n",
        "\n",
        "            # --- CAPACITY & DWELL ---\n",
        "            # Disembark\n",
        "            want_to_get_off = current_train_load * disembarking_percentages[station]\n",
        "            current_train_load -= want_to_get_off\n",
        "            if current_train_load < 0: current_train_load = 0\n",
        "\n",
        "            # Board\n",
        "            space_available = train_capacity - current_train_load\n",
        "\n",
        "            if total_waiting <= space_available:\n",
        "                actually_boarding = total_waiting\n",
        "                leftover = 0\n",
        "            else:\n",
        "                actually_boarding = space_available\n",
        "                leftover = total_waiting - space_available\n",
        "\n",
        "            current_train_load += actually_boarding\n",
        "            station_leftover_passengers[station] = leftover\n",
        "\n",
        "            # Dwell Time\n",
        "            time_to_disembark = want_to_get_off / boarding_speed\n",
        "            time_to_board = actually_boarding / boarding_speed\n",
        "\n",
        "            raw_dwell = max(time_to_disembark, time_to_board)\n",
        "            dwell_time = math.ceil(raw_dwell)\n",
        "\n",
        "            # --- DEPARTURE ---\n",
        "            departure_time = actual_boarding_start + dwell_time\n",
        "            planned_departures[(train, station)] = departure_time\n",
        "\n",
        "            station_free_time[station] = departure_time\n",
        "            natural_arrival_time = departure_time\n",
        "\n",
        "    # --- OUTPUT GENERATION ---\n",
        "    output = []\n",
        "    output.append(\"non-fluents nf_simple_train_model{\\n\")\n",
        "    output.append(\"    domain = train_system;\\n\\n\")\n",
        "\n",
        "    output.append(\"    objects{\\n\")\n",
        "    output.append(f\"        train : {{{', '.join(trains)}}};\\n\")\n",
        "    output.append(f\"        station : {{{', '.join(stations)}}};\\n\")\n",
        "    output.append(\"    };\\n\\n\")\n",
        "\n",
        "    output.append(\"    non-fluents{\\n\")\n",
        "\n",
        "    for i in range(num_trains):\n",
        "        next_train = trains[(i + 1) % num_trains]\n",
        "        output.append(f\"        NEXT_TRAIN({trains[i]}, {next_train}) = true;\\n\")\n",
        "\n",
        "    output.append(f\"\\n        DEPOT_STATION({stations[-1]}) = true;\\n\\n\")\n",
        "\n",
        "    for i in range(len(stations) - 1):\n",
        "        output.append(f\"        NEXT_STATION({stations[i]}, {stations[i+1]}) = true;\\n\")\n",
        "        output.append(f\"        FIND_NEXT_STATION({stations[i]}) = {stations[i+1]};\\n\")\n",
        "\n",
        "    output.append(\n",
        "     f\"        FIND_NEXT_STATION({stations[-1]}) = {stations[-1]};\\n\"\n",
        "    )\n",
        "\n",
        "    output.append(f\"        FIRST_TRAIN = {trains[0]};\\n\")\n",
        "\n",
        "\n",
        "    output.append(\"\\n\")\n",
        "    for (s1, s2), time in drive_times.items():\n",
        "        output.append(f\"        DRIVE_TIME({s1}, {s2}) = {time};\\n\")\n",
        "\n",
        "    output.append(\"\\n\")\n",
        "    for station, percentage in disembarking_percentages.items():\n",
        "        output.append(f\"        DISEMBARKING_PRECENTAGE({station}) = {percentage:.2f};\\n\")\n",
        "\n",
        "    output.append(\"\\n\")\n",
        "    for station, rate in passenger_arrival_rates.items():\n",
        "        output.append(f\"        PASSENGER_ARRIVAL_RATE({station}) = {rate};\\n\")\n",
        "\n",
        "    output.append(\"\\n\")\n",
        "    for (train, station), time in planned_departures.items():\n",
        "        output.append(f\"        PLANNED_DEPARTURE_TIME({train}, {station}) = {int(time)};\\n\")\n",
        "\n",
        "    output.append(\"\\n    };\\n\")\n",
        "    output.append(\"}\\n\\n\")\n",
        "\n",
        "    output.append(\"instance simple_train_model{\\n\")\n",
        "    output.append(\"    domain = train_system;\\n\")\n",
        "    output.append(\"    non-fluents = nf_simple_train_model;\\n\\n\")\n",
        "\n",
        "    output.append(\"    init-state{\\n\")\n",
        "    for i, train in enumerate(trains):\n",
        "        output.append(f\"        train_timer({train}) = {train_timers[i]};\\n\")\n",
        "\n",
        "    output.append(\"\\n\")\n",
        "    for train in trains:\n",
        "        output.append(f\"        current_state({train}) = {0};\\n\")\n",
        "        output.append(f\"        current_station({train}) = {stations[0]};\\n\")\n",
        "\n",
        "    output.append(\"    };\\n\\n\")\n",
        "    output.append(f\"    max-nondef-actions = pos-inf;\\n\")\n",
        "    output.append(f\"    horizon = {horizon};\\n\")\n",
        "    output.append(\"    discount = 1.0;\\n\\n\")\n",
        "    output.append(\"}\\n\")\n",
        "\n",
        "    return \"\".join(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXsXVy7spld1"
      },
      "source": [
        "#### instance generator - usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p12cIU5Ipq0Y",
        "outputId": "7d2dfff3-5c76-4d0b-fe7f-4b82a7d85be8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully generated RDDL instance at: /content/instance.rddl\n",
            "\n",
            "\n",
            "non-fluents nf_simple_train_model{\n",
            "    domain = train_system;\n",
            "\n",
            "    objects{\n",
            "        train : {t1, t2, t3};\n",
            "        station : {s1, s2, s3, s4};\n",
            "    };\n",
            "\n",
            "    non-fluents{\n",
            "        NEXT_TRAIN(t1, t2) = true;\n",
            "        NEXT_TRAIN(t2, t3) = true;\n",
            "        NEXT_TRAIN(t3, t1) = true;\n",
            "\n",
            "        DEPOT_STATION(s4) = true;\n",
            "\n",
            "        NEXT_STATION(s1, s2) = true;\n",
            "        FIND_NEXT_STATION(s1) = s2;\n",
            "        NEXT_STATION(s2, s3) = true;\n",
            "        FIND_NEXT_STATION(s2) = s3;\n",
            "        NEXT_STATION(s3, s4) = true;\n",
            "        FIND_NEXT_STATION(s3) = s4;\n",
            "        FIND_NEXT_STATION(s4) = s4;\n",
            "        FIRST_TRAIN = t1;\n",
            "\n",
            "        DRIVE_TIME(s1, s2) = 45;\n",
            "        DRIVE_TIME(s2, s3) = 53;\n",
            "        DRIVE_TIME(s3, s4) = 36;\n",
            "\n",
            "        DISEMBARKING_PRECENTAGE(s1) = 0.12;\n",
            "        DISEMBARKING_PRECENTAGE(s2) = 0.35;\n",
            "        DISEMBARKING_PRECENTAGE(s3) = 0.24;\n",
            "        DISEMBARKING_PRECENTAGE(s4) = 0.29;\n",
            "\n",
            "        PASSENGER_ARRIVAL_RATE(s1) = 8;\n",
            "        PASSENGER_ARRIVAL_RATE(s2) = 5;\n",
            "        PASSENGER_ARRIVAL_RATE(s3) = 2;\n",
            "        PASSENGER_ARRIVAL_RATE(s4) = 0;\n",
            "\n",
            "        PLANNED_DEPARTURE_TIME(t1, s1) = 17;\n",
            "        PLANNED_DEPARTURE_TIME(t1, s2) = 67;\n",
            "        PLANNED_DEPARTURE_TIME(t1, s3) = 124;\n",
            "        PLANNED_DEPARTURE_TIME(t1, s4) = 163;\n",
            "        PLANNED_DEPARTURE_TIME(t2, s1) = 49;\n",
            "        PLANNED_DEPARTURE_TIME(t2, s2) = 97;\n",
            "        PLANNED_DEPARTURE_TIME(t2, s3) = 151;\n",
            "        PLANNED_DEPARTURE_TIME(t2, s4) = 189;\n",
            "        PLANNED_DEPARTURE_TIME(t3, s1) = 79;\n",
            "        PLANNED_DEPARTURE_TIME(t3, s2) = 126;\n",
            "        PLANNED_DEPARTURE_TIME(t3, s3) = 180;\n",
            "        PLANNED_DEPARTURE_TIME(t3, s4) = 218;\n",
            "\n",
            "    };\n",
            "}\n",
            "\n",
            "instance simple_train_model{\n",
            "    domain = train_system;\n",
            "    non-fluents = nf_simple_train_model;\n",
            "\n",
            "    init-state{\n",
            "        train_timer(t1) = 15;\n",
            "        train_timer(t2) = 45;\n",
            "        train_timer(t3) = 75;\n",
            "\n",
            "        current_state(t1) = 0;\n",
            "        current_station(t1) = s1;\n",
            "        current_state(t2) = 0;\n",
            "        current_station(t2) = s1;\n",
            "        current_state(t3) = 0;\n",
            "        current_station(t3) = s1;\n",
            "    };\n",
            "\n",
            "    max-nondef-actions = pos-inf;\n",
            "    horizon = 50;\n",
            "    discount = 1.0;\n",
            "\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "instance_file_path = base_path + 'instance.rddl'\n",
        "\n",
        "horizon = 50\n",
        "\n",
        "rddl_instance = generate_pyrddlgym_instance(\n",
        "    num_trains=3,\n",
        "    num_stations=4,\n",
        "    horizon=horizon,\n",
        "    interval=30,\n",
        "    start_delay=15\n",
        ")\n",
        "\n",
        "with open(instance_file_path, 'w') as instance_file:\n",
        "    instance_file.write(rddl_instance)\n",
        "\n",
        "print(f\"Successfully generated RDDL instance at: {instance_file_path}\")\n",
        "\n",
        "print(\"\\n\\n\" + rddl_instance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjs5NFDsmcHZ"
      },
      "source": [
        "## RDDL usage NEW - Our custom Visualizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kO8iER4B0lGS"
      },
      "source": [
        "###Viz implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGh_JzCv0oWm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle, Circle\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
        "from PIL import Image\n",
        "\n",
        "from pyRDDLGym.core.compiler.model import RDDLPlanningModel\n",
        "\n",
        "class TrainRouteVisualizer:\n",
        "    def __init__(self, model):\n",
        "        self._model = model\n",
        "\n",
        "        # Reduced figsize and set DPI to avoid exceeding maximum image size\n",
        "        self.fig, self.ax = plt.subplots(figsize=(5, 5), dpi=80)  # Adjusted figsize and DPI\n",
        "        self.route_radius = 0.4  # Set the route radius\n",
        "        self.train_radius = self.route_radius - 0.1  # Slightly inside the route\n",
        "        self.station_size = 0.05  # Adjust station square size as needed\n",
        "        self.train_size = 0.01  # Adjust train size\n",
        "        self.x_center = 0.5  # Center of the route\n",
        "        self.y_center = 0.5  # Center of the route\n",
        "        self._nonfluents = model.ground_vars_with_values(model.non_fluents)  # Load all non-fluents\n",
        "\n",
        "        self.train_patches = {}  # Store train circles by train ID\n",
        "\n",
        "        # Create the route circle\n",
        "        self.create_route_circle()\n",
        "\n",
        "        #store drive time to stations\n",
        "        self.drive_time_to_dest = {}\n",
        "        for k, v in self._nonfluents.items():\n",
        "          var, objects = RDDLPlanningModel.parse_grounded(k)\n",
        "          if var == 'DRIVE_TIME':\n",
        "            _, dest = objects  # Only care about the destination\n",
        "            if v > 0:\n",
        "              self.drive_time_to_dest[dest] = v  # Store time to destination\n",
        "\n",
        "        #store depot station\n",
        "        for k, v in self._nonfluents.items():\n",
        "          var, objects = RDDLPlanningModel.parse_grounded(k)\n",
        "          if var == 'DEPOT_STATION':\n",
        "             if v:\n",
        "               self.depot_station = objects[0]  # Store the depot station\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_drive_time_to_dest(self, dest):\n",
        "      return self.drive_time_to_dest.get(dest, -1)  # Default to -1 if not found\n",
        "\n",
        "    def create_route_circle(self):\n",
        "        \"\"\"Draw the circular route.\"\"\"\n",
        "        route_circle = plt.Circle((self.x_center, self.y_center), self.route_radius, color='blue', fill=False, linestyle='dashed')\n",
        "        self.ax.add_patch(route_circle)\n",
        "\n",
        "\n",
        "    #set the station color such that it shows the crowding level\n",
        "    def set_station_color(self, station, state):\n",
        "      capacity = 1000\n",
        "      passengers_at_station = state.get(f\"passengers_at_station___{station}\", 0)\n",
        "      crowding_level = passengers_at_station / capacity\n",
        "\n",
        "      cmap = plt.get_cmap('RdYlGn_r')\n",
        "      color = cmap(crowding_level)\n",
        "      return color\n",
        "\n",
        "    #set the \"s1/s2/s3..\" text, so it will be seen regardless to station color\n",
        "    def set_text_color(self, station_color):\n",
        "      r, g, b, *_ = map(float, station_color)\n",
        "      luminance = 0.299 * r + 0.587 * g + 0.114 * b  # Calculate luminance\n",
        "\n",
        "      # Return black for bright colors, white for dark colors\n",
        "      return 'black' if luminance > 0.5 else 'white'\n",
        "\n",
        "\n",
        "\n",
        "    def draw_stations(self, state):\n",
        "\n",
        "        \"\"\"Draw stations equally spaced along the route.\"\"\"\n",
        "        stations = self._model.type_to_objects['station']\n",
        "        num_stations = len(stations)\n",
        "        delta_theta = 2 * np.pi / num_stations  # Angle between stations\n",
        "\n",
        "\n",
        "        for i, station in enumerate(stations):\n",
        "\n",
        "            theta = i * delta_theta  # Compute station angle\n",
        "            x = self.x_center + self.route_radius * np.cos(theta)\n",
        "            y = self.y_center + self.route_radius * np.sin(theta)\n",
        "\n",
        "            if station == self.depot_station:\n",
        "              station_color = 'black'\n",
        "              text_color = 'white'\n",
        "              self.ax.text(x , y -0.1 , \"Depot\", fontsize=10, ha='center', va='bottom', color='red')\n",
        "\n",
        "            else:\n",
        "              station_color = self.set_station_color(station, state)\n",
        "              text_color = self.set_text_color(station_color)\n",
        "\n",
        "            # Create and draw station (square)\n",
        "            square = Rectangle((x - self.station_size / 2, y - self.station_size / 2),\n",
        "                               self.station_size, self.station_size, color = station_color)\n",
        "            self.ax.add_patch(square)\n",
        "\n",
        "            # Add station number inside the station\n",
        "            self.ax.text(x , y - self.station_size / 2, f\"{station}\", fontsize=10, ha='center', va='bottom', color = text_color)\n",
        "\n",
        "\n",
        "    def draw_train(self, last_train_state, train_state, train, station, station_theta, delta_theta, train_timer, num_in_queue):\n",
        "        \"\"\"Draw the train on the route based on its state.\"\"\"\n",
        "\n",
        "        #delete last draw of train\n",
        "        if self.train_patches.get(train) is not None:\n",
        "          old_patch, old_text = self.train_patches[train]\n",
        "          old_patch.remove()  # Removes the train's old circle from the figure\n",
        "          old_text.remove()  # Remove previous train label\n",
        "          del self.train_patches[train]\n",
        "\n",
        "        #delete trains at the end\n",
        "        if train_state == 'train_finished':\n",
        "          return\n",
        "\n",
        "        #if train in now in station, draw it in station\n",
        "        if train_state in ['train_waiting_at_station', 'train_active_at_station']:\n",
        "          x = self.x_center + self.route_radius * np.cos(station_theta)\n",
        "          y = self.y_center +  self.route_radius * np.sin(station_theta)\n",
        "          train_circle = Circle((x, y), self.train_size)\n",
        "          x_text = x + 0.07*np.cos(station_theta)\n",
        "          y_text = y + 0.07*np.sin(station_theta)\n",
        "          train_text = self.ax.text(x_text, y_text, f\"{train}\", fontsize=10, ha='left', va='center', color='black')\n",
        "          self.ax.add_patch(train_circle)\n",
        "          self.train_patches[train] = (train_circle, train_text)\n",
        "\n",
        "\n",
        "        if train_state == 'train_num_at_queue':\n",
        "\n",
        "          x = self.x_center + (self.route_radius - 0.05*num_in_queue) * np.cos(station_theta)\n",
        "          y = self.y_center +  (self.route_radius - 0.05*num_in_queue) * np.sin(station_theta)\n",
        "\n",
        "          train_circle = Circle((x, y), self.train_size)\n",
        "          x_text = x + 0.065*np.cos(station_theta + np.pi/2)\n",
        "          y_text = y + 0.065*np.sin(station_theta + np.pi/2)\n",
        "          train_text = self.ax.text(x_text, y_text, f\"{train}\", fontsize=10, ha='left', va='center', color='black')\n",
        "          self.ax.add_patch(train_circle)\n",
        "          self.train_patches[train] = (train_circle, train_text)\n",
        "\n",
        "\n",
        "\n",
        "        if train_state == 'train_in_route':\n",
        "\n",
        "          #if the train is in route to the first station, and the time is longer than the\n",
        "\n",
        "          total_drive_time = self.get_drive_time_to_dest(station)\n",
        "          progress_theta = delta_theta / total_drive_time;\n",
        "\n",
        "          if(train_timer <= total_drive_time):\n",
        "\n",
        "            x = self.x_center + self.route_radius * np.cos(station_theta - progress_theta* (train_timer))\n",
        "            y = self.y_center +  self.route_radius * np.sin(station_theta - progress_theta*(train_timer))\n",
        "\n",
        "            train_circle = Circle((x, y), self.train_size)\n",
        "            train_text = self.ax.text(x + 0.05, y, f\"{train}\", fontsize=10, ha='left', va='center', color='black')\n",
        "            self.ax.add_patch(train_circle)\n",
        "            self.train_patches[train] = (train_circle, train_text)\n",
        "\n",
        "\n",
        "\n",
        "    def render(self, state):\n",
        "        \"\"\"Render the train route visualization.\"\"\"\n",
        "        trains = self._model.type_to_objects['train']\n",
        "        stations = self._model.type_to_objects['station']\n",
        "        num_stations = len(stations)\n",
        "        delta_theta = 2 * np.pi / num_stations  # Angle between stations\n",
        "\n",
        "        self.draw_stations(state)\n",
        "\n",
        "\n",
        "        last_train_state = {train: None for train in trains}\n",
        "        # Iterate through trains and stations\n",
        "        for train in trains:\n",
        "\n",
        "          # Determine the state of the train for this station\n",
        "          train_state = None\n",
        "          if state.get(f\"current_state___{train}\") == 0:\n",
        "              train_state = 'train_in_route'\n",
        "          elif state.get(f\"train_num_at_queue___{train}\") > 0:\n",
        "              train_state = 'train_num_at_queue'\n",
        "          elif state.get(f\"current_state___{train}\") == 2:\n",
        "              train_state = 'train_waiting_at_station'\n",
        "          elif state.get(f\"current_state___{train}\") == 3:\n",
        "              train_state = 'train_active_at_station'\n",
        "          elif state.get(f\"current_state___{train}\") == 4:\n",
        "              train_state = 'train_finished'\n",
        "\n",
        "          station = state.get(f\"current_station___{train}\")\n",
        "\n",
        "          if train_state:\n",
        "\n",
        "\n",
        "              # Compute station angle on the circle\n",
        "              station_index = stations.index(station)\n",
        "              station_theta = station_index * delta_theta\n",
        "\n",
        "              #train timer\n",
        "              train_timer = state.get(f\"train_timer___{train}\", 0)\n",
        "\n",
        "              #train num at queue\n",
        "              num_in_queue = state.get(f\"train_num_at_queue___{train}\", 0)\n",
        "\n",
        "\n",
        "              # Draw train\n",
        "              self.draw_train(last_train_state, train_state, train, station, station_theta, delta_theta, train_timer, num_in_queue)\n",
        "              last_train_state[train] = train_state\n",
        "\n",
        "        # Finalize and return image\n",
        "        self.fig.canvas.draw()\n",
        "        img = self.convert2img(self.fig.canvas)\n",
        "        return img\n",
        "\n",
        "    def convert2img(self, canvas):\n",
        "        \"\"\"Convert Matplotlib figure to a PIL Image.\"\"\"\n",
        "        canvas.draw()\n",
        "        buf, (width, height) = canvas.print_to_buffer()\n",
        "        img = np.frombuffer(buf, dtype=np.uint8).reshape((height, width, 4))  # RGBA format\n",
        "        return Image.fromarray(img)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otaVw8hBUTmN"
      },
      "source": [
        "###our Environment Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_rgz022UbFf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "044f6a4d-e167-4f17-af10-dd91c6fb370f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "\n",
        "class RDDLDecisionWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.trains = env.model.type_to_objects['train']\n",
        "        self.stations = env.model.type_to_objects['station']\n",
        "\n",
        "        self.TRAIN_IN_ROUTE = 0\n",
        "        self.TRAIN_IN_QUEUE = 1\n",
        "        self.TRAIN_WAITING = 2\n",
        "        self.TRAIN_ACTIVE = 3\n",
        "\n",
        "        self.full_logs = []\n",
        "        self.last_obs = None\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.full_logs = []\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        self.last_obs = obs.copy()\n",
        "\n",
        "        self.full_logs.append({\n",
        "            'step_type': 'RESET',\n",
        "            'state': obs.copy()\n",
        "        })\n",
        "\n",
        "        if not self.get_active_stations(obs):\n",
        "            obs, _, terminated, truncated, info = self._skip_forward(0, False, False, info)\n",
        "\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "        pre_step_obs = self.last_obs.copy()\n",
        "\n",
        "        # 1. FILTER ACTION\n",
        "        active_stations = self.get_active_stations(pre_step_obs)\n",
        "\n",
        "        env_action = {}\n",
        "        log_action = {}\n",
        "\n",
        "        for s in self.stations:\n",
        "            key = f'wait___{s}'\n",
        "\n",
        "            if s in active_stations:\n",
        "                val = action.get(key, 0)\n",
        "                env_action[key] = val\n",
        "                log_action[key] = val\n",
        "            else:\n",
        "                env_action[key] = 0\n",
        "                log_action[key] = None\n",
        "\n",
        "        # 2. Apply ENV Action\n",
        "        obs, reward, terminated, truncated, info = self.env.step(env_action)\n",
        "        self.last_obs = obs.copy()\n",
        "\n",
        "        # Inject LOG Action\n",
        "        info['filtered_action'] = log_action\n",
        "\n",
        "        self.full_logs.append({\n",
        "            'step_type': 'AGENT_ACTION',\n",
        "            'state': pre_step_obs,\n",
        "            'action': log_action,\n",
        "            'original_action': action,\n",
        "            'next_state': obs.copy(),\n",
        "            'reward': reward\n",
        "        })\n",
        "\n",
        "        # 3. Skipping Loop\n",
        "        if not terminated and not truncated:\n",
        "            obs, extra_reward, terminated, truncated, info = self._skip_forward(reward, terminated, truncated, info)\n",
        "            reward = extra_reward\n",
        "\n",
        "        info['filtered_action'] = log_action\n",
        "\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "    def _skip_forward(self, current_reward, terminated, truncated, info):\n",
        "        \"\"\"Helper to run the skipping loop.\"\"\"\n",
        "        done = terminated or truncated\n",
        "        total_reward = current_reward\n",
        "        obs = self.last_obs\n",
        "\n",
        "        while not done and not self.get_active_stations(obs):\n",
        "\n",
        "            pre_internal_obs = self.last_obs.copy()\n",
        "            default_action = {f'wait___{s}': 0 for s in self.stations}\n",
        "\n",
        "            obs, reward, terminated, truncated, info = self.env.step(default_action)\n",
        "            self.last_obs = obs.copy()\n",
        "\n",
        "            self.full_logs.append({\n",
        "                'step_type': 'INTERNAL_SKIP',\n",
        "                'state': pre_internal_obs,\n",
        "                'action': default_action,\n",
        "                'next_state': obs.copy(),\n",
        "                'reward': reward\n",
        "            })\n",
        "\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "\n",
        "        return obs, total_reward, terminated, truncated, info\n",
        "\n",
        "    def get_active_stations(self, obs):\n",
        "        active_set = set()\n",
        "        current_timers = {t: obs[f'train_timer___{t}'] for t in self.trains}\n",
        "        valid_timers = [t for t in current_timers.values() if t < 1e9]\n",
        "\n",
        "        if not valid_timers: return set()\n",
        "        global_timer = min(valid_timers)\n",
        "\n",
        "        station_occupant = {s: None for s in self.stations}\n",
        "        station_has_queue = {s: False for s in self.stations}\n",
        "\n",
        "        for t in self.trains:\n",
        "            s = obs[f'current_station___{t}']\n",
        "            state = obs[f'current_state___{t}']\n",
        "            if state == self.TRAIN_IN_QUEUE:\n",
        "                station_has_queue[s] = True\n",
        "            elif state in [self.TRAIN_WAITING, self.TRAIN_ACTIVE]:\n",
        "                station_occupant[s] = t\n",
        "\n",
        "        for t in self.trains:\n",
        "            timer = current_timers[t]\n",
        "            state = obs[f'current_state___{t}']\n",
        "            station = obs[f'current_station___{t}']\n",
        "\n",
        "            if timer == global_timer:\n",
        "                if state == self.TRAIN_IN_ROUTE:\n",
        "                    occupant = station_occupant[station]\n",
        "                    if occupant is None:\n",
        "                        active_set.add(station)\n",
        "                    else:\n",
        "                        occupant_state = obs[f'current_state___{occupant}']\n",
        "                        occupant_timer = current_timers[occupant]\n",
        "                        if (occupant_state == self.TRAIN_ACTIVE and\n",
        "                            occupant_timer == global_timer and\n",
        "                            not station_has_queue[station]):\n",
        "                            active_set.add(station)\n",
        "\n",
        "                if state == self.TRAIN_ACTIVE and station_has_queue[station]:\n",
        "                    active_set.add(station)\n",
        "\n",
        "        return active_set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYT9__qfs70m"
      },
      "source": [
        "##PPO implementation and usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt989B6LqyPv"
      },
      "source": [
        "###PPO adapter wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iikyugYsq3R3"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "\n",
        "class PPOAdapter(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        sample_obs, _ = self.env.reset()\n",
        "        self.flat_size = self._dict_to_vec(sample_obs).shape[0]\n",
        "        print(f\"PPO Adapter Initialized. Flattened State Size: {self.flat_size}\")\n",
        "\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.flat_size,), dtype=np.float32)\n",
        "        self.action_space = spaces.Discrete(11)\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        return self._dict_to_vec(obs), info\n",
        "\n",
        "    def step(self, action_int):\n",
        "        action_dict = {}\n",
        "        for s in self.env.stations:\n",
        "            action_dict[f'wait___{s}'] = int(action_int)\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action_dict)\n",
        "        return self._dict_to_vec(obs), reward, terminated, truncated, info\n",
        "\n",
        "    def _dict_to_vec(self, obs_dict):\n",
        "        values = []\n",
        "        for key in sorted(obs_dict.keys()):\n",
        "            val = obs_dict[key]\n",
        "            if isinstance(val, str) or np.issubdtype(type(val), np.str_):\n",
        "                try:\n",
        "                    num = float(''.join(filter(str.isdigit, str(val))))\n",
        "                    values.append(num)\n",
        "                except ValueError: values.append(0.0)\n",
        "            elif np.isscalar(val) or (isinstance(val, np.ndarray) and val.ndim == 0):\n",
        "                try: values.append(float(val))\n",
        "                except: values.append(0.0)\n",
        "            elif isinstance(val, np.ndarray):\n",
        "                values.extend(val.flatten().astype(float))\n",
        "            else: values.append(0.0)\n",
        "        return np.array(values, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QW24sKC-tBP-"
      },
      "source": [
        "###PPO algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iat0V-TCtESe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78250a2d-7ffe-4a23-b39b-ab29cbde76d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Memory:\n",
        "    def __init__(self):\n",
        "        self.actions, self.states, self.logprobs, self.rewards, self.is_terminals = [], [], [], [], []\n",
        "    def clear(self):\n",
        "        del self.actions[:], self.states[:], self.logprobs[:], self.rewards[:], self.is_terminals[:]\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, n_latent_var):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.action_layer = nn.Sequential(\n",
        "            nn.Linear(state_dim, n_latent_var), nn.Tanh(),\n",
        "            nn.Linear(n_latent_var, n_latent_var), nn.Tanh(),\n",
        "            nn.Linear(n_latent_var, action_dim), nn.Softmax(dim=-1)\n",
        "        )\n",
        "        self.value_layer = nn.Sequential(\n",
        "            nn.Linear(state_dim, n_latent_var), nn.Tanh(),\n",
        "            nn.Linear(n_latent_var, n_latent_var), nn.Tanh(),\n",
        "            nn.Linear(n_latent_var, 1)\n",
        "        )\n",
        "    def act(self, state, memory):\n",
        "        state = torch.from_numpy(state).float().to(device)\n",
        "        action_probs = self.action_layer(state)\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "        memory.states.append(state)\n",
        "        memory.actions.append(action)\n",
        "        memory.logprobs.append(dist.log_prob(action))\n",
        "        return action.item()\n",
        "    def evaluate(self, state, action):\n",
        "        action_probs = self.action_layer(state)\n",
        "        dist = Categorical(action_probs)\n",
        "        action_logprobs = dist.log_prob(action)\n",
        "        dist_entropy = dist.entropy()\n",
        "        state_value = self.value_layer(state)\n",
        "        return action_logprobs, torch.squeeze(state_value), dist_entropy\n",
        "\n",
        "class PPO:\n",
        "    def __init__(self, state_dim, action_dim, lr, betas, gamma, K_epochs, eps_clip):\n",
        "        self.lr, self.betas, self.gamma, self.eps_clip, self.K_epochs = lr, betas, gamma, eps_clip, K_epochs\n",
        "        self.policy = ActorCritic(state_dim, action_dim, 64).to(device)\n",
        "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\n",
        "        self.policy_old = ActorCritic(state_dim, action_dim, 64).to(device)\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "        self.MseLoss = nn.MSELoss()\n",
        "\n",
        "    def select_action(self, state, memory):\n",
        "        return self.policy_old.act(state, memory)\n",
        "\n",
        "    def update(self, memory):\n",
        "        rewards = []\n",
        "        discounted_reward = 0\n",
        "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
        "            if is_terminal: discounted_reward = 0\n",
        "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
        "            rewards.insert(0, discounted_reward)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
        "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
        "        old_states = torch.stack(memory.states).to(device).detach()\n",
        "        old_actions = torch.stack(memory.actions).to(device).detach()\n",
        "        old_logprobs = torch.stack(memory.logprobs).to(device).detach()\n",
        "\n",
        "        for _ in range(self.K_epochs):\n",
        "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
        "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
        "            advantages = rewards - state_values.detach()\n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
        "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "    # --- UPDATED SAVE FUNCTION ---\n",
        "    def save_checkpoint(self, path, episode_num):\n",
        "        torch.save({\n",
        "            'model_state': self.policy_old.state_dict(),\n",
        "            'optimizer_state': self.optimizer.state_dict(),\n",
        "            'episode': episode_num\n",
        "        }, path)\n",
        "\n",
        "    # --- UPDATED LOAD FUNCTION ---\n",
        "    def load_checkpoint(self, path):\n",
        "        print(f\"Loading checkpoint from {path}...\")\n",
        "        checkpoint = torch.load(path, map_location=device)\n",
        "\n",
        "        # Load weights\n",
        "        self.policy.load_state_dict(checkpoint['model_state'])\n",
        "        self.policy_old.load_state_dict(checkpoint['model_state'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
        "\n",
        "        # Return the saved episode number\n",
        "        return checkpoint.get('episode', 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2G49-uUtgjf"
      },
      "source": [
        "###ppo interaction loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfhY6B4GS4Nd",
        "outputId": "37d3b872-249e-4e14-c348-2e8a0c1423cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Checkpoints will be saved to: /content/drive/My Drive/Train_Scheduler_PPO\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# 1. Mount Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Define Save Path\n",
        "checkpoint_dir = '/content/drive/My Drive/Train_Scheduler_PPO'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Checkpoints will be saved to: {checkpoint_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pqe9a-vJtjIO",
        "outputId": "ea28c987-254d-4a16-9a3e-7a3b836afd1a"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Token 'DOT' defined, but not used\n",
            "WARNING: Token 'QUESTION' defined, but not used\n",
            "WARNING: Token 'TERMINAL' defined, but not used\n",
            "WARNING: There are 3 unused tokens\n",
            "Generating LALR tables\n",
            "WARNING: 1 shift/reduce conflict\n",
            "WARNING: 6 reduce/reduce conflicts\n",
            "WARNING: reduce/reduce conflict in state 353 resolved using rule (term -> VAR)\n",
            "WARNING: rejected rule (pvar_expr -> VAR) in state 353\n",
            "WARNING: reduce/reduce conflict in state 354 resolved using rule (term -> ENUM_VAL)\n",
            "WARNING: rejected rule (pvar_expr -> ENUM_VAL) in state 354\n",
            "/usr/local/lib/python3.12/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PPO Adapter Initialized. Flattened State Size: 20\n",
            "Loading checkpoint from /content/drive/My Drive/Train_Scheduler_PPO/latest_model.pth...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RESUMING TRAINING FROM EPISODE 2151\n",
            "STARTING PPO TRAINING ON cpu...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pyRDDLGym/core/debug/exception.py:28: UserWarning: Removed 1 temporary files at /content/Train_model_*_temp.png.\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   [NEW RECORD] Best Reward: -23.00. Saving best model...\n",
            "   [NEW RECORD] Best Reward: -19.00. Saving best model...\n",
            "   [NEW RECORD] Best Reward: -14.00. Saving best model...\n",
            "   [NEW RECORD] Best Reward: -11.00. Saving best model...\n",
            "   [NEW RECORD] Best Reward: -8.00. Saving best model...\n",
            "Episode 2160 \t Avg Reward: -10.20 \t Best: -8.00\n",
            "Episode 2180 \t Avg Reward: -27.40 \t Best: -8.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 2200)...\n",
            "Episode 2200 \t Avg Reward: -24.50 \t Best: -8.00\n",
            "Episode 2220 \t Avg Reward: -26.30 \t Best: -8.00\n",
            "Episode 2240 \t Avg Reward: -25.40 \t Best: -8.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 2250)...\n",
            "Episode 2260 \t Avg Reward: -24.90 \t Best: -8.00\n",
            "Episode 2280 \t Avg Reward: -23.90 \t Best: -8.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 2300)...\n",
            "Episode 2300 \t Avg Reward: -27.90 \t Best: -8.00\n",
            "   [UPDATE] Learning Step (Total Timesteps: 2000)...\n",
            "Episode 2320 \t Avg Reward: -23.25 \t Best: -8.00\n",
            "Episode 2340 \t Avg Reward: -20.70 \t Best: -8.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 2350)...\n",
            "Episode 2360 \t Avg Reward: -23.05 \t Best: -8.00\n",
            "Episode 2380 \t Avg Reward: -20.50 \t Best: -8.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 2400)...\n",
            "Episode 2400 \t Avg Reward: -22.90 \t Best: -8.00\n",
            "Episode 2420 \t Avg Reward: -22.60 \t Best: -8.00\n",
            "Episode 2440 \t Avg Reward: -22.10 \t Best: -8.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 2450)...\n",
            "Episode 2460 \t Avg Reward: -18.95 \t Best: -8.00\n",
            "Episode 2480 \t Avg Reward: -23.80 \t Best: -8.00\n",
            "   [UPDATE] Learning Step (Total Timesteps: 2000)...\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 2500)...\n",
            "Episode 2500 \t Avg Reward: -21.90 \t Best: -8.00\n",
            "Episode 2520 \t Avg Reward: -20.05 \t Best: -8.00\n",
            "Episode 2540 \t Avg Reward: -19.10 \t Best: -8.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 2550)...\n",
            "Episode 2560 \t Avg Reward: -23.25 \t Best: -8.00\n",
            "Episode 2580 \t Avg Reward: -22.05 \t Best: -8.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 2600)...\n",
            "Episode 2600 \t Avg Reward: -20.55 \t Best: -8.00\n",
            "Episode 2620 \t Avg Reward: -22.35 \t Best: -8.00\n",
            "Episode 2640 \t Avg Reward: -19.75 \t Best: -8.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 2650)...\n",
            "   [UPDATE] Learning Step (Total Timesteps: 2000)...\n",
            "Episode 2660 \t Avg Reward: -20.05 \t Best: -8.00\n",
            "Episode 2680 \t Avg Reward: -18.55 \t Best: -8.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 2700)...\n",
            "Episode 2700 \t Avg Reward: -19.95 \t Best: -8.00\n",
            "Episode 2720 \t Avg Reward: -24.00 \t Best: -8.00\n",
            "Episode 2740 \t Avg Reward: -20.25 \t Best: -8.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 2750)...\n",
            "Episode 2760 \t Avg Reward: -19.30 \t Best: -8.00\n",
            "Episode 2780 \t Avg Reward: -21.70 \t Best: -8.00\n",
            "   [NEW RECORD] Best Reward: -6.00. Saving best model...\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 2800)...\n",
            "Episode 2800 \t Avg Reward: -16.90 \t Best: -6.00\n",
            "Episode 2820 \t Avg Reward: -19.95 \t Best: -6.00\n",
            "   [UPDATE] Learning Step (Total Timesteps: 2000)...\n",
            "Episode 2840 \t Avg Reward: -16.85 \t Best: -6.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 2850)...\n",
            "Episode 2860 \t Avg Reward: -20.00 \t Best: -6.00\n",
            "Episode 2880 \t Avg Reward: -18.75 \t Best: -6.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 2900)...\n",
            "Episode 2900 \t Avg Reward: -19.90 \t Best: -6.00\n",
            "Episode 2920 \t Avg Reward: -20.30 \t Best: -6.00\n",
            "Episode 2940 \t Avg Reward: -19.30 \t Best: -6.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 2950)...\n",
            "Episode 2960 \t Avg Reward: -18.00 \t Best: -6.00\n",
            "Episode 2980 \t Avg Reward: -18.80 \t Best: -6.00\n",
            "   [UPDATE] Learning Step (Total Timesteps: 2000)...\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 3000)...\n",
            "Episode 3000 \t Avg Reward: -18.55 \t Best: -6.00\n",
            "Episode 3020 \t Avg Reward: -17.00 \t Best: -6.00\n",
            "Episode 3040 \t Avg Reward: -16.70 \t Best: -6.00\n",
            "   [NEW RECORD] Best Reward: -5.00. Saving best model...\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 3050)...\n",
            "Episode 3060 \t Avg Reward: -17.05 \t Best: -5.00\n",
            "Episode 3080 \t Avg Reward: -17.20 \t Best: -5.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 3100)...\n",
            "Episode 3100 \t Avg Reward: -17.60 \t Best: -5.00\n",
            "Episode 3120 \t Avg Reward: -19.10 \t Best: -5.00\n",
            "Episode 3140 \t Avg Reward: -18.90 \t Best: -5.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 3150)...\n",
            "   [UPDATE] Learning Step (Total Timesteps: 2000)...\n",
            "Episode 3160 \t Avg Reward: -18.15 \t Best: -5.00\n",
            "Episode 3180 \t Avg Reward: -16.95 \t Best: -5.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 3200)...\n",
            "Episode 3200 \t Avg Reward: -15.60 \t Best: -5.00\n",
            "Episode 3220 \t Avg Reward: -17.20 \t Best: -5.00\n",
            "Episode 3240 \t Avg Reward: -19.35 \t Best: -5.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 3250)...\n",
            "Episode 3260 \t Avg Reward: -17.95 \t Best: -5.00\n",
            "Episode 3280 \t Avg Reward: -20.55 \t Best: -5.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 3300)...\n",
            "Episode 3300 \t Avg Reward: -17.15 \t Best: -5.00\n",
            "Episode 3320 \t Avg Reward: -15.95 \t Best: -5.00\n",
            "   [UPDATE] Learning Step (Total Timesteps: 2000)...\n",
            "Episode 3340 \t Avg Reward: -14.35 \t Best: -5.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 3350)...\n",
            "Episode 3360 \t Avg Reward: -15.60 \t Best: -5.00\n",
            "Episode 3380 \t Avg Reward: -17.40 \t Best: -5.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 3400)...\n",
            "Episode 3400 \t Avg Reward: -15.65 \t Best: -5.00\n",
            "Episode 3420 \t Avg Reward: -16.30 \t Best: -5.00\n",
            "Episode 3440 \t Avg Reward: -15.50 \t Best: -5.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 3450)...\n",
            "Episode 3460 \t Avg Reward: -16.35 \t Best: -5.00\n",
            "Episode 3480 \t Avg Reward: -14.25 \t Best: -5.00\n",
            "   [UPDATE] Learning Step (Total Timesteps: 2000)...\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 3500)...\n",
            "Episode 3500 \t Avg Reward: -17.65 \t Best: -5.00\n",
            "Episode 3520 \t Avg Reward: -15.30 \t Best: -5.00\n",
            "Episode 3540 \t Avg Reward: -16.90 \t Best: -5.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 3550)...\n",
            "Episode 3560 \t Avg Reward: -15.75 \t Best: -5.00\n",
            "Episode 3580 \t Avg Reward: -16.65 \t Best: -5.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 3600)...\n",
            "Episode 3600 \t Avg Reward: -14.40 \t Best: -5.00\n",
            "Episode 3620 \t Avg Reward: -15.70 \t Best: -5.00\n",
            "Episode 3640 \t Avg Reward: -15.55 \t Best: -5.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 3650)...\n",
            "   [UPDATE] Learning Step (Total Timesteps: 2000)...\n",
            "Episode 3660 \t Avg Reward: -15.10 \t Best: -5.00\n",
            "Episode 3680 \t Avg Reward: -14.55 \t Best: -5.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 3700)...\n",
            "Episode 3700 \t Avg Reward: -13.70 \t Best: -5.00\n",
            "Episode 3720 \t Avg Reward: -14.95 \t Best: -5.00\n",
            "Episode 3740 \t Avg Reward: -15.70 \t Best: -5.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 3750)...\n",
            "Episode 3760 \t Avg Reward: -14.30 \t Best: -5.00\n",
            "Episode 3780 \t Avg Reward: -13.95 \t Best: -5.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 3800)...\n",
            "Episode 3800 \t Avg Reward: -16.55 \t Best: -5.00\n",
            "Episode 3820 \t Avg Reward: -14.85 \t Best: -5.00\n",
            "   [UPDATE] Learning Step (Total Timesteps: 2000)...\n",
            "Episode 3840 \t Avg Reward: -13.50 \t Best: -5.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 3850)...\n",
            "Episode 3860 \t Avg Reward: -15.90 \t Best: -5.00\n",
            "Episode 3880 \t Avg Reward: -14.65 \t Best: -5.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 3900)...\n",
            "Episode 3900 \t Avg Reward: -14.55 \t Best: -5.00\n",
            "Episode 3920 \t Avg Reward: -15.55 \t Best: -5.00\n",
            "Episode 3940 \t Avg Reward: -16.20 \t Best: -5.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 3950)...\n",
            "Episode 3960 \t Avg Reward: -13.80 \t Best: -5.00\n",
            "Episode 3980 \t Avg Reward: -14.50 \t Best: -5.00\n",
            "   [UPDATE] Learning Step (Total Timesteps: 2000)...\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 4000)...\n",
            "Episode 4000 \t Avg Reward: -15.25 \t Best: -5.00\n",
            "Episode 4020 \t Avg Reward: -13.10 \t Best: -5.00\n",
            "Episode 4040 \t Avg Reward: -14.75 \t Best: -5.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 4050)...\n",
            "Episode 4060 \t Avg Reward: -13.65 \t Best: -5.00\n",
            "Episode 4080 \t Avg Reward: -15.05 \t Best: -5.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 4100)...\n",
            "Episode 4100 \t Avg Reward: -13.25 \t Best: -5.00\n",
            "Episode 4120 \t Avg Reward: -12.95 \t Best: -5.00\n",
            "Episode 4140 \t Avg Reward: -12.90 \t Best: -5.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 4150)...\n",
            "   [UPDATE] Learning Step (Total Timesteps: 2000)...\n",
            "Episode 4160 \t Avg Reward: -13.85 \t Best: -5.00\n",
            "Episode 4180 \t Avg Reward: -12.65 \t Best: -5.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 4200)...\n",
            "Episode 4200 \t Avg Reward: -12.85 \t Best: -5.00\n",
            "Episode 4220 \t Avg Reward: -13.25 \t Best: -5.00\n",
            "Episode 4240 \t Avg Reward: -12.85 \t Best: -5.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 4250)...\n",
            "Episode 4260 \t Avg Reward: -14.45 \t Best: -5.00\n",
            "Episode 4280 \t Avg Reward: -13.50 \t Best: -5.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 4300)...\n",
            "Episode 4300 \t Avg Reward: -15.05 \t Best: -5.00\n",
            "Episode 4320 \t Avg Reward: -13.35 \t Best: -5.00\n",
            "   [UPDATE] Learning Step (Total Timesteps: 2000)...\n",
            "Episode 4340 \t Avg Reward: -11.90 \t Best: -5.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 4350)...\n",
            "Episode 4360 \t Avg Reward: -13.25 \t Best: -5.00\n",
            "Episode 4380 \t Avg Reward: -14.05 \t Best: -5.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 4400)...\n",
            "Episode 4400 \t Avg Reward: -14.45 \t Best: -5.00\n",
            "Episode 4420 \t Avg Reward: -13.15 \t Best: -5.00\n",
            "Episode 4440 \t Avg Reward: -12.55 \t Best: -5.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 4450)...\n",
            "Episode 4460 \t Avg Reward: -13.95 \t Best: -5.00\n",
            "Episode 4480 \t Avg Reward: -13.60 \t Best: -5.00\n",
            "   [UPDATE] Learning Step (Total Timesteps: 2000)...\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 4500)...\n",
            "Episode 4500 \t Avg Reward: -11.95 \t Best: -5.00\n",
            "Episode 4520 \t Avg Reward: -12.10 \t Best: -5.00\n",
            "Episode 4540 \t Avg Reward: -13.35 \t Best: -5.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 4550)...\n",
            "Episode 4560 \t Avg Reward: -12.70 \t Best: -5.00\n",
            "Episode 4580 \t Avg Reward: -13.85 \t Best: -5.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 4600)...\n",
            "Episode 4600 \t Avg Reward: -11.25 \t Best: -5.00\n",
            "Episode 4620 \t Avg Reward: -12.45 \t Best: -5.00\n",
            "Episode 4640 \t Avg Reward: -12.95 \t Best: -5.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 4650)...\n",
            "   [UPDATE] Learning Step (Total Timesteps: 2000)...\n",
            "Episode 4660 \t Avg Reward: -12.35 \t Best: -5.00\n",
            "Episode 4680 \t Avg Reward: -13.15 \t Best: -5.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 4700)...\n",
            "Episode 4700 \t Avg Reward: -11.20 \t Best: -5.00\n",
            "Episode 4720 \t Avg Reward: -13.65 \t Best: -5.00\n",
            "Episode 4740 \t Avg Reward: -11.65 \t Best: -5.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 4750)...\n",
            "Episode 4760 \t Avg Reward: -13.20 \t Best: -5.00\n",
            "Episode 4780 \t Avg Reward: -13.05 \t Best: -5.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 4800)...\n",
            "Episode 4800 \t Avg Reward: -12.65 \t Best: -5.00\n",
            "Episode 4820 \t Avg Reward: -12.60 \t Best: -5.00\n",
            "   [UPDATE] Learning Step (Total Timesteps: 2000)...\n",
            "Episode 4840 \t Avg Reward: -14.00 \t Best: -5.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 4850)...\n",
            "Episode 4860 \t Avg Reward: -13.35 \t Best: -5.00\n",
            "Episode 4880 \t Avg Reward: -13.10 \t Best: -5.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 4900)...\n",
            "Episode 4900 \t Avg Reward: -12.55 \t Best: -5.00\n",
            "Episode 4920 \t Avg Reward: -12.70 \t Best: -5.00\n",
            "Episode 4940 \t Avg Reward: -12.30 \t Best: -5.00\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 4950)...\n",
            "Episode 4960 \t Avg Reward: -12.05 \t Best: -5.00\n",
            "Episode 4980 \t Avg Reward: -12.50 \t Best: -5.00\n",
            "   [UPDATE] Learning Step (Total Timesteps: 2000)...\n",
            "   [CHECKPOINT] Saving latest model to Drive (Episode 5000)...\n",
            "Episode 5000 \t Avg Reward: -12.55 \t Best: -5.00\n",
            "TRAINING COMPLETE!\n"
          ]
        }
      ],
      "source": [
        "from pyRDDLGym.core.visualizer.movie import MovieGenerator\n",
        "import os\n",
        "\n",
        "# --- CONTROL PANEL ---\n",
        "FORCE_NEW_RUN = False  # Set to True to ignore saved files and start fresh\n",
        "# ---------------------\n",
        "\n",
        "# 1. Setup Environment\n",
        "myEnv = pyRDDLGym.make(domain=base_path+'domain.rddl', instance=base_path+'instance.rddl')\n",
        "ENV = 'Train_model'\n",
        "MovieGen = MovieGenerator(base_path, ENV, horizon)\n",
        "myEnv.set_visualizer(TrainRouteVisualizer, movie_gen=MovieGen, movie_per_episode=True)\n",
        "myEnv = RDDLDecisionWrapper(myEnv)\n",
        "myEnv = PPOAdapter(myEnv)\n",
        "\n",
        "# 2. Hyperparameters\n",
        "state_dim = myEnv.observation_space.shape[0]\n",
        "action_dim = 11\n",
        "lr = 0.002\n",
        "betas = (0.9, 0.999)\n",
        "gamma = 0.99\n",
        "K_epochs = 4\n",
        "eps_clip = 0.2\n",
        "update_timestep = 2000\n",
        "\n",
        "memory = Memory()\n",
        "ppo = PPO(state_dim, action_dim, lr, betas, gamma, K_epochs, eps_clip)\n",
        "\n",
        "# 3. CHECK FOR SAVED FILES & RESUME\n",
        "latest_model_path = os.path.join(checkpoint_dir, 'latest_model.pth')\n",
        "best_model_path = os.path.join(checkpoint_dir, 'best_model.pth')\n",
        "\n",
        "start_episode = 1\n",
        "\n",
        "if FORCE_NEW_RUN:\n",
        "    print(\"!!! FORCE_NEW_RUN is True. Starting fresh (ignoring any saved checkpoints).\")\n",
        "elif os.path.exists(latest_model_path):\n",
        "    # Load weights AND get the episode number\n",
        "    loaded_ep = ppo.load_checkpoint(latest_model_path)\n",
        "    start_episode = loaded_ep + 1\n",
        "    print(f\"RESUMING TRAINING FROM EPISODE {start_episode}\")\n",
        "else:\n",
        "    print(\"No checkpoint found. Starting fresh from Episode 1.\")\n",
        "\n",
        "# 4. Training Loop\n",
        "running_reward = 0\n",
        "time_step = 0\n",
        "log_interval = 20\n",
        "save_interval = 50\n",
        "max_episodes = 5000\n",
        "best_reward = -float('inf')\n",
        "\n",
        "print(f\"STARTING PPO TRAINING ON {device}...\")\n",
        "\n",
        "# --- UPDATED LOOP RANGE ---\n",
        "for i_episode in range(start_episode, max_episodes+1):\n",
        "    state, _ = myEnv.reset()\n",
        "    current_ep_reward = 0\n",
        "\n",
        "    for t in range(myEnv.env.horizon):\n",
        "        time_step += 1\n",
        "\n",
        "        action = ppo.select_action(state, memory)\n",
        "        state, reward, done, truncated, info = myEnv.step(action)\n",
        "\n",
        "        memory.rewards.append(reward)\n",
        "        memory.is_terminals.append(done)\n",
        "        current_ep_reward += reward\n",
        "\n",
        "        if time_step % update_timestep == 0:\n",
        "            print(f\"   [UPDATE] Learning Step (Total Timesteps: {time_step})...\")\n",
        "            ppo.update(memory)\n",
        "            memory.clear()\n",
        "            time_step = 0\n",
        "\n",
        "        if done or truncated: break\n",
        "\n",
        "    running_reward += current_ep_reward\n",
        "\n",
        "    # 5. LOGGING AND SAVING\n",
        "    if current_ep_reward > best_reward:\n",
        "        best_reward = current_ep_reward\n",
        "        print(f\"   [NEW RECORD] Best Reward: {best_reward:.2f}. Saving best model...\")\n",
        "        # Save best model with current episode\n",
        "        ppo.save_checkpoint(best_model_path, i_episode)\n",
        "\n",
        "    # Periodic Save (Standard Checkpoint)\n",
        "    if i_episode % save_interval == 0:\n",
        "        print(f\"   [CHECKPOINT] Saving latest model to Drive (Episode {i_episode})...\")\n",
        "        ppo.save_checkpoint(latest_model_path, i_episode)\n",
        "\n",
        "    if i_episode % log_interval == 0:\n",
        "        avg_reward = running_reward / log_interval\n",
        "        print(f'Episode {i_episode} \\t Avg Reward: {avg_reward:.2f} \\t Best: {best_reward:.2f}')\n",
        "        running_reward = 0\n",
        "\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "myEnv.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmD7xO6DtShG"
      },
      "source": [
        "##Debug environment (our old interaction loop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHBFhM1KX6gj"
      },
      "source": [
        "### Interaction loop - Our viz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIq9jE12X90B"
      },
      "outputs": [],
      "source": [
        "# import sys\n",
        "# import time\n",
        "# import numpy as np\n",
        "# from pyRDDLGym.core.visualizer.movie import MovieGenerator\n",
        "# from pyRDDLGym.core.policy import RandomAgent\n",
        "\n",
        "# # 1. Setup Environment\n",
        "# # IMPORTANT: This must be called AFTER Block 1 has written the new domain.rddl file\n",
        "# myEnv = pyRDDLGym.make(domain=base_path+'domain.rddl', instance=base_path+'instance.rddl')\n",
        "# ENV = 'Train_model'\n",
        "# MovieGen = MovieGenerator(base_path, ENV, horizon)\n",
        "# myEnv.set_visualizer(TrainRouteVisualizer, movie_gen=MovieGen, movie_per_episode=True)\n",
        "# myEnv = RDDLDecisionWrapper(myEnv)\n",
        "\n",
        "# # 2. Agent\n",
        "# agent = RandomAgent(action_space=myEnv.action_space,\n",
        "#                         num_actions=myEnv.max_allowed_actions,\n",
        "#                         seed=43)\n",
        "\n",
        "# gif_name =  ENV+'_vizexample'\n",
        "# total_reward = 0\n",
        "# state, _ = myEnv.reset()\n",
        "\n",
        "# agent_view_log = \"\"\n",
        "# bar_length = 50\n",
        "# total_steps = myEnv.horizon\n",
        "# step = 0\n",
        "\n",
        "# # 3. Run Loop\n",
        "# while True:\n",
        "#     myEnv.render(to_display=False)\n",
        "\n",
        "#     action = agent.sample_action()\n",
        "\n",
        "#     prev_state = state.copy()\n",
        "\n",
        "#     next_state, reward, terminated, truncated, info = myEnv.step(action)\n",
        "\n",
        "#     real_action = info.get('filtered_action', action)\n",
        "\n",
        "#     total_reward += reward\n",
        "\n",
        "#     agent_view_log += f'\\nstep       = {step}\\n'\n",
        "#     agent_view_log += f'state      = {prev_state}\\n'\n",
        "#     agent_view_log += f'action     = {real_action}\\n'\n",
        "#     agent_view_log += f'next state = {next_state}\\n'\n",
        "#     agent_view_log += f'reward     = {reward}\\n'\n",
        "\n",
        "#     state = next_state\n",
        "#     step += 1\n",
        "\n",
        "#     percent = 100 * (step / float(total_steps))\n",
        "#     filled_length = int(bar_length * step // total_steps)\n",
        "#     bar = '█' * filled_length + '░' * (bar_length - filled_length)\n",
        "#     sys.stdout.write(f'\\r|{bar}| {percent:.1f}%')\n",
        "#     sys.stdout.flush()\n",
        "\n",
        "#     if terminated or truncated:\n",
        "#         break\n",
        "\n",
        "# agent_view_log += f'\\nEpisode ended with Total Reward: {total_reward}\\n'\n",
        "# sys.stdout.write(f'\\r|{\"█\" * bar_length}| 100.0%\\n')\n",
        "\n",
        "# myEnv.close()\n",
        "\n",
        "# # --- SAVE LOGS ---\n",
        "# with open('output.txt', 'w') as f:\n",
        "#     f.write(agent_view_log)\n",
        "\n",
        "# full_log_str = \"FULL DEBUG LOG (Includes Skipped Steps)\\n=======================================\\n\"\n",
        "# for i, entry in enumerate(myEnv.full_logs):\n",
        "#     full_log_str += f\"\\n[Index {i}] Type: {entry['step_type']}\\n\"\n",
        "#     if 'state' in entry: full_log_str += f\"state      = {entry['state']}\\n\"\n",
        "#     if 'action' in entry: full_log_str += f\"action     = {entry['action']}\\n\"\n",
        "#     if 'next_state' in entry: full_log_str += f\"next state = {entry['next_state']}\\n\"\n",
        "#     if 'reward' in entry: full_log_str += f\"reward     = {entry['reward']}\\n\"\n",
        "\n",
        "# with open('full_debug_log.txt', 'w') as f:\n",
        "#     f.write(full_log_str)\n",
        "\n",
        "# print(\"\\nLogs generated: 'output.txt' and 'full_debug_log.txt'\")\n",
        "\n",
        "# from moviepy.editor import VideoFileClip\n",
        "# MovieGen.save_animation(gif_name)\n",
        "# clip = VideoFileClip(base_path + gif_name + '.gif')\n",
        "# clip.ipython_display()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AgI-Z_ANqos"
      },
      "source": [
        "###Agent view log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0ntgswxhSEp"
      },
      "outputs": [],
      "source": [
        "# with open('output.txt', 'r') as f: # Open the file in read mode\n",
        "#   contents = f.read() # Read the entire contents of the file\n",
        "#   print(contents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOAv7DIpNx2k"
      },
      "source": [
        "###Full debug log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_uG8xtuN1-9"
      },
      "outputs": [],
      "source": [
        "# with open('full_debug_log.txt', 'r') as f: # Open the file in read mode\n",
        "#   contents = f.read() # Read the entire contents of the file\n",
        "#   print(contents)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "88EvRVo3GZ6D",
        "MgxqsGNpokOl",
        "rsf8EJjop5bG",
        "3CU3O3LNoWls",
        "dXsXVy7spld1",
        "kO8iER4B0lGS",
        "otaVw8hBUTmN",
        "dmD7xO6DtShG",
        "RHBFhM1KX6gj",
        "2AgI-Z_ANqos",
        "AOAv7DIpNx2k"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}